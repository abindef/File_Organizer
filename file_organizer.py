import os
import shutil
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import argparse
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
try:
    from PIL import Image
    from PIL.ExifTags import TAGS
    PILLOW_AVAILABLE = True
except ImportError:
    PILLOW_AVAILABLE = False


class FileOrganizer:
    def __init__(self, source_dir, target_dir=None, dry_run=False, remove_duplicates=False, include_brand=False, max_workers=4):
        """
        åˆå§‹åŒ–æ–‡ä»¶æ•´ç†å™¨
        
        Args:
            source_dir: æºæ–‡ä»¶å¤¹è·¯å¾„
            target_dir: ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™åœ¨æºæ–‡ä»¶å¤¹ä¸‹åˆ›å»ºorganizedæ–‡ä»¶å¤¹
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…ç§»åŠ¨æ–‡ä»¶
            remove_duplicates: æ˜¯å¦åˆ é™¤é‡å¤æ–‡ä»¶
            include_brand: æ˜¯å¦åœ¨æ–‡ä»¶åä¸­åŒ…å«ç›¸æœºå“ç‰Œ
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
        """
        self.source_dir = Path(source_dir)
        self.target_dir = Path(target_dir) if target_dir else self.source_dir / "organized"
        self.dry_run = dry_run
        self.remove_duplicates = remove_duplicates
        self.include_brand = include_brand
        self.max_workers = max_workers
        self.lock = threading.Lock()
        self.processed_count = 0
        self.error_count = 0
        self.failed_files = []
        self.failed_dir = self.target_dir.parent / "failed_files" if target_dir else self.source_dir / "failed_files"
        
        if not self.source_dir.exists():
            raise ValueError(f"æºæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {self.source_dir}")
        
        if self.include_brand and not PILLOW_AVAILABLE:
            print("è­¦å‘Š: æœªå®‰è£…Pillowåº“ï¼Œæ— æ³•æå–ç›¸æœºå“ç‰Œä¿¡æ¯ã€‚è¯·è¿è¡Œ: pip install Pillow")
            self.include_brand = False
    
    def get_file_modified_date(self, file_path):
        """è·å–æ–‡ä»¶çš„æœ€åä¿®æ”¹æ—¶é—´"""
        timestamp = os.path.getmtime(file_path)
        return datetime.fromtimestamp(timestamp)
    
    def extract_camera_brand(self, file_path):
        """
        ä»å›¾ç‰‡EXIFæ•°æ®ä¸­æå–ç›¸æœºå“ç‰Œ
        
        Args:
            file_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
        
        Returns:
            ç›¸æœºå“ç‰Œåç§°ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
        """
        if not PILLOW_AVAILABLE:
            return None
        
        try:
            image = Image.open(file_path)
            exif_data = image._getexif()
            
            if exif_data:
                for tag_id, value in exif_data.items():
                    tag = TAGS.get(tag_id, tag_id)
                    if tag == "Make":
                        # æ¸…ç†å“ç‰Œåç§°ï¼Œç§»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
                        brand = str(value).strip()
                        # ç§»é™¤å¸¸è§çš„åç¼€
                        brand = brand.replace('CORPORATION', '').replace('Corporation', '').strip()
                        # æ¸…ç†éæ³•å­—ç¬¦
                        brand = self.sanitize_filename(brand)
                        return brand if brand else None
        except Exception:
            pass
        
        return None
    
    def sanitize_filename(self, filename):
        """
        æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ç©ºå­—ç¬¦å’Œå…¶ä»–éæ³•å­—ç¬¦
        
        Args:
            filename: åŸå§‹æ–‡ä»¶å
        
        Returns:
            æ¸…ç†åçš„æ–‡ä»¶å
        """
        if not filename:
            return "unnamed"
        
        # ç§»é™¤ç©ºå­—ç¬¦å’Œå…¶ä»–æ§åˆ¶å­—ç¬¦ï¼ˆASCII < 32ï¼‰
        sanitized = ''.join(char for char in filename if ord(char) >= 32 and char != '\x00')
        
        # ç§»é™¤Windowsæ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
        invalid_chars = '<>:"|?*\\/\r\n\t'
        for char in invalid_chars:
            sanitized = sanitized.replace(char, '_')
        
        # ç§»é™¤å‰åç©ºæ ¼å’Œç‚¹
        sanitized = sanitized.strip('. ')
        
        # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
        if not sanitized:
            return "unnamed"
        
        return sanitized
    
    def calculate_file_hash(self, file_path, block_size=65536):
        """
        è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            block_size: è¯»å–å—å¤§å°
        
        Returns:
            æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼
        """
        md5 = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                while True:
                    data = f.read(block_size)
                    if not data:
                        break
                    md5.update(data)
            return md5.hexdigest()
        except Exception as e:
            print(f"  è­¦å‘Š: æ— æ³•è®¡ç®—æ–‡ä»¶å“ˆå¸Œ {file_path.name} - {e}")
            return None
    
    def scan_files_recursive(self):
        """
        é€’å½’æ‰«ææ‰€æœ‰æ–‡ä»¶ï¼ˆåŒ…æ‹¬å­æ–‡ä»¶å¤¹ï¼‰ï¼Œå¸¦é”™è¯¯å¤„ç†
        
        Returns:
            æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        files = []
        error_count = 0
        
        def scan_directory(path):
            nonlocal error_count
            try:
                for item in path.iterdir():
                    try:
                        if item.is_file():
                            # éªŒè¯æ–‡ä»¶è·¯å¾„æ˜¯å¦æœ‰æ•ˆ
                            try:
                                item.stat()
                                files.append(item)
                            except (OSError, PermissionError):
                                error_count += 1
                        elif item.is_dir():
                            scan_directory(item)
                    except (OSError, PermissionError):
                        error_count += 1
            except (OSError, PermissionError):
                error_count += 1
        
        scan_directory(self.source_dir)
        
        if error_count > 0:
            print(f"\nè­¦å‘Š: {error_count} ä¸ªè·¯å¾„æ— æ³•è®¿é—®ï¼Œå·²è‡ªåŠ¨è·³è¿‡")
        
        return files
    
    def calculate_hash_worker(self, file_path):
        """å¤šçº¿ç¨‹å“ˆå¸Œè®¡ç®—å·¥ä½œå‡½æ•°"""
        try:
            file_hash = self.calculate_file_hash(file_path)
            return (file_path, file_hash)
        except Exception as e:
            return (file_path, None)
    
    def find_duplicates(self):
        """
        æŸ¥æ‰¾é‡å¤æ–‡ä»¶ï¼ˆå¤šçº¿ç¨‹ï¼‰
        
        Returns:
            å­—å…¸ï¼Œkeyä¸ºæ–‡ä»¶å“ˆå¸Œï¼Œvalueä¸ºå…·æœ‰ç›¸åŒå“ˆå¸Œçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        hash_map = defaultdict(list)
        
        print("\næ­£åœ¨é€’å½’æ‰«ææ–‡ä»¶å¹¶è®¡ç®—å“ˆå¸Œå€¼...")
        all_files = self.scan_files_recursive()
        total_files = len(all_files)
        print(f"æ‰¾åˆ° {total_files} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è®¡ç®—å“ˆå¸Œå€¼...")
        
        file_count = 0
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self.calculate_hash_worker, f): f for f in all_files}
            
            for future in as_completed(futures):
                file_path, file_hash = future.result()
                if file_hash:
                    hash_map[file_hash].append(file_path)
                    file_count += 1
                    
                    if file_count % 1000 == 0:
                        print(f"  å·²å¤„ç† {file_count}/{total_files} ä¸ªæ–‡ä»¶...")
        
        print(f"å·²æ‰«æ {file_count} ä¸ªæ–‡ä»¶")
        
        duplicates = {k: v for k, v in hash_map.items() if len(v) > 1}
        return duplicates
    
    def remove_duplicate_files(self):
        """
        åˆ é™¤é‡å¤æ–‡ä»¶ï¼Œä¿ç•™æœ€æ—©çš„æ–‡ä»¶ï¼ˆæ ¹æ®ä¿®æ”¹æ—¶é—´ï¼‰
        
        Returns:
            åˆ é™¤çš„æ–‡ä»¶æ•°é‡
        """
        duplicates = self.find_duplicates()
        
        if not duplicates:
            print("\næœªå‘ç°é‡å¤æ–‡ä»¶")
            return 0
        
        total_duplicates = sum(len(files) - 1 for files in duplicates.values())
        print(f"\nå‘ç° {len(duplicates)} ç»„é‡å¤æ–‡ä»¶ï¼Œå…± {total_duplicates} ä¸ªé‡å¤é¡¹")
        
        if self.dry_run:
            print("\nã€é¢„è§ˆæ¨¡å¼ã€‘ä»¥ä¸‹æ–‡ä»¶å°†è¢«åˆ é™¤:\n")
        else:
            print("\næ­£åœ¨åˆ é™¤é‡å¤æ–‡ä»¶...\n")
        
        deleted_count = 0
        total_size_saved = 0
        
        for file_hash, files in duplicates.items():
            files_with_dates = [(f, self.get_file_modified_date(f)) for f in files]
            files_with_dates.sort(key=lambda x: x[1])
            
            keep_file = files_with_dates[0][0]
            duplicates_to_remove = files_with_dates[1:]
            
            print(f"ä¿ç•™: {keep_file.name} ({files_with_dates[0][1].strftime('%Y-%m-%d %H:%M:%S')})")
            
            for dup_file, dup_date in duplicates_to_remove:
                file_size = dup_file.stat().st_size
                print(f"  åˆ é™¤: {dup_file.name} ({dup_date.strftime('%Y-%m-%d %H:%M:%S')}) - {self.format_size(file_size)}")
                
                if not self.dry_run:
                    try:
                        dup_file.unlink()
                        deleted_count += 1
                        total_size_saved += file_size
                    except Exception as e:
                        print(f"    é”™è¯¯: åˆ é™¤å¤±è´¥ - {e}")
            print()
        
        if not self.dry_run:
            print(f"å®Œæˆ! åˆ é™¤äº† {deleted_count} ä¸ªé‡å¤æ–‡ä»¶ï¼ŒèŠ‚çœç©ºé—´ {self.format_size(total_size_saved)}")
        else:
            estimated_size = sum(f.stat().st_size for files in duplicates.values() for f in files[1:])
            print(f"é¢„è§ˆå®Œæˆ! å°†åˆ é™¤ {total_duplicates} ä¸ªé‡å¤æ–‡ä»¶ï¼Œé¢„è®¡èŠ‚çœç©ºé—´ {self.format_size(estimated_size)}")
        
        return deleted_count
    
    def find_available_sequence(self, month_dir, date, extension, start_seq=1, brand=None):
        """
        æŸ¥æ‰¾å¯ç”¨çš„åºå·ï¼Œå¦‚æœæ–‡ä»¶å·²å­˜åœ¨åˆ™è‡ªåŠ¨é€’å¢
        
        Args:
            month_dir: æœˆä»½ç›®å½•
            date: æ—¥æœŸå¯¹è±¡
            extension: æ–‡ä»¶æ‰©å±•å
            start_seq: èµ·å§‹åºå·
            brand: ç›¸æœºå“ç‰Œï¼ˆå¯é€‰ï¼‰
        
        Returns:
            å¯ç”¨çš„åºå·å’Œå¯¹åº”çš„æ–‡ä»¶å
        """
        seq = start_seq
        while True:
            new_filename = self.generate_new_filename(date, seq, extension, brand)
            new_path = month_dir / new_filename
            if not new_path.exists():
                return seq, new_filename
            seq += 1
            if seq > 999:
                raise ValueError(f"åºå·è¶…å‡ºèŒƒå›´ï¼ˆæœ€å¤§999ï¼‰: {date.strftime('%Y%m%d')}")
    
    @staticmethod
    def format_size(size_bytes):
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.2f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.2f} PB"
    
    def process_file_info(self, file_path):
        """å¤šçº¿ç¨‹æ–‡ä»¶ä¿¡æ¯å¤„ç†å·¥ä½œå‡½æ•°"""
        try:
            modified_date = self.get_file_modified_date(file_path)
            year_month = (modified_date.year, modified_date.month)
            
            brand = None
            if self.include_brand:
                brand = self.extract_camera_brand(file_path)
            
            return {
                'path': file_path,
                'date': modified_date,
                'extension': file_path.suffix,
                'brand': brand,
                'year_month': year_month
            }
        except Exception as e:
            with self.lock:
                self.failed_files.append((str(file_path), str(e)))
            return None
    
    def group_files_by_date(self):
        """æŒ‰å¹´æœˆåˆ†ç»„æ–‡ä»¶ï¼ˆé€’å½’æ‰«ææ‰€æœ‰å­æ–‡ä»¶å¤¹ï¼Œå¤šçº¿ç¨‹å¤„ç†ï¼‰"""
        files_by_date = defaultdict(list)
        
        print("\næ­£åœ¨é€’å½’æ‰«ææ–‡ä»¶å¤¹...")
        all_files = self.scan_files_recursive()
        total_files = len(all_files)
        print(f"æ‰¾åˆ° {total_files} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹åˆ†æ...")
        
        processed = 0
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self.process_file_info, f): f for f in all_files}
            
            for future in as_completed(futures):
                file_info = future.result()
                if file_info:
                    year_month = file_info.pop('year_month')
                    files_by_date[year_month].append(file_info)
                    processed += 1
                    
                    if processed % 1000 == 0:
                        print(f"  å·²åˆ†æ {processed}/{total_files} ä¸ªæ–‡ä»¶...")
        
        print(f"åˆ†æå®Œæˆï¼Œå…± {processed} ä¸ªæ–‡ä»¶")
        
        for year_month in files_by_date:
            files_by_date[year_month].sort(key=lambda x: x['date'])
        
        return files_by_date
    
    
    def generate_new_filename(self, date, sequence, extension, brand=None):
        """
        ç”Ÿæˆæ–°æ–‡ä»¶å
        
        Args:
            date: datetimeå¯¹è±¡
            sequence: åºå·
            extension: æ–‡ä»¶æ‰©å±•å
            brand: ç›¸æœºå“ç‰Œï¼ˆå¯é€‰ï¼‰
        
        Returns:
            æ–°æ–‡ä»¶åï¼Œæ ¼å¼ä¸º Brand_yyyymmdd00x.ext æˆ– yyyymmdd00x.ext
        """
        date_str = date.strftime('%Y%m%d')
        seq_str = f"{sequence:03d}"
        
        # æ¸…ç†æ‰©å±•å
        extension = self.sanitize_filename(extension) if extension else ""
        if extension and not extension.startswith('.'):
            extension = '.' + extension
        
        # æ¸…ç†å“ç‰Œåç§°
        if brand:
            brand = self.sanitize_filename(brand)
            return f"{brand}_{date_str}{seq_str}{extension}"
        
        return f"{date_str}{seq_str}{extension}"
    
    def organize_files(self):
        """æ‰§è¡Œæ–‡ä»¶æ•´ç†"""
        # å…ˆæ•´ç†æ–‡ä»¶
        files_by_date = self.group_files_by_date()
        
        if not files_by_date:
            print("æœªæ‰¾åˆ°éœ€è¦æ•´ç†çš„æ–‡ä»¶")
            return
        
        total_files = sum(len(files) for files in files_by_date.values())
        print(f"æ‰¾åˆ° {total_files} ä¸ªæ–‡ä»¶éœ€è¦æ•´ç†")
        print(f"ç›®æ ‡ç›®å½•: {self.target_dir}")
        
        if self.dry_run:
            print("\nã€é¢„è§ˆæ¨¡å¼ã€‘ä¸ä¼šå®é™…ç§»åŠ¨æ–‡ä»¶\n")
        
        moved_count = 0
        conflict_count = 0
        
        for (year, month), files in sorted(files_by_date.items()):
            year_dir = self.target_dir / str(year)
            month_dir = year_dir / f"{month:02d}"
            
            print(f"\nå¤„ç† {year}å¹´{month}æœˆ çš„ {len(files)} ä¸ªæ–‡ä»¶...")
            
            if not self.dry_run:
                month_dir.mkdir(parents=True, exist_ok=True)
            
            files_by_day = defaultdict(list)
            for file_info in files:
                day = file_info['date'].day
                files_by_day[day].append(file_info)
            
            file_index = 0
            for day, day_files in sorted(files_by_day.items()):
                for seq, file_info in enumerate(day_files, start=1):
                    old_path = file_info['path']
                    brand = file_info.get('brand')
                    
                    actual_seq, new_filename = self.find_available_sequence(
                        month_dir,
                        file_info['date'],
                        file_info['extension'],
                        seq,
                        brand
                    )
                    new_path = month_dir / new_filename
                    
                    file_index += 1
                    # æ¯1000ä¸ªæ–‡ä»¶æ‰“å°ä¸€æ¬¡æ—¥å¿—
                    if file_index % 1000 == 0:
                        print(f"  å·²å¤„ç† {file_index}/{len(files)} ä¸ªæ–‡ä»¶...")
                    elif actual_seq != seq:
                        print(f"  åºå·å†²çªï¼Œè‡ªåŠ¨è°ƒæ•´: {old_path.name} -> {year}/{month:02d}/{new_filename} (åºå· {seq} -> {actual_seq})")
                        conflict_count += 1
                    
                    if not self.dry_run:
                        try:
                            shutil.move(str(old_path), str(new_path))
                            moved_count += 1
                        except Exception as e:
                            print(f"  é”™è¯¯: ç§»åŠ¨æ–‡ä»¶å¤±è´¥ - {e}")
                            with self.lock:
                                self.failed_files.append((str(old_path), str(e)))
        
        if not self.dry_run:
            print(f"\nå®Œæˆ! æˆåŠŸæ•´ç† {moved_count} ä¸ªæ–‡ä»¶")
            if conflict_count > 0:
                print(f"å…¶ä¸­ {conflict_count} ä¸ªæ–‡ä»¶å› åºå·å†²çªè‡ªåŠ¨è°ƒæ•´äº†åºå·")
            
            # å¤„ç†å¤±è´¥çš„æ–‡ä»¶
            if self.failed_files:
                self.handle_failed_files()
            
            # æ•´ç†å®Œæˆåå†åˆ é™¤é‡å¤æ–‡ä»¶
            if self.remove_duplicates:
                print("\n" + "="*50)
                print("æ•´ç†å®Œæˆï¼Œå¼€å§‹æ£€æµ‹å¹¶åˆ é™¤é‡å¤æ–‡ä»¶...")
                print("="*50 + "\n")
                self.remove_duplicates_from_organized()
            
            # æ˜¾ç¤ºä»»åŠ¡å®Œæˆæ€»ç»“
            print("\n" + "="*60)
            print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")
            print("="*60)
            print(f"âœ… æˆåŠŸæ•´ç†: {moved_count} ä¸ªæ–‡ä»¶")
            if conflict_count > 0:
                print(f"âš ï¸  åºå·è°ƒæ•´: {conflict_count} ä¸ªæ–‡ä»¶")
            if self.failed_files:
                print(f"âŒ å¤„ç†å¤±è´¥: {len(self.failed_files)} ä¸ªæ–‡ä»¶")
            print(f"ğŸ“ ç›®æ ‡ç›®å½•: {self.target_dir}")
            print("="*60)
        else:
            print(f"\né¢„è§ˆå®Œæˆ! å…± {total_files} ä¸ªæ–‡ä»¶å°†è¢«æ•´ç†")
            if conflict_count > 0:
                print(f"å…¶ä¸­ {conflict_count} ä¸ªæ–‡ä»¶å°†å› åºå·å†²çªè‡ªåŠ¨è°ƒæ•´åºå·")
            print("\næç¤º: ä½¿ç”¨ä¸å¸¦ --dry-run å‚æ•°è¿è¡Œä»¥å®é™…æ‰§è¡Œæ•´ç†æ“ä½œ")
    
    def handle_failed_files(self):
        """å¤„ç†å¤±è´¥çš„æ–‡ä»¶ï¼Œç§»åŠ¨åˆ°failed_filesæ–‡ä»¶å¤¹"""
        if not self.failed_files:
            return
        
        print(f"\nå¤„ç† {len(self.failed_files)} ä¸ªå¤±è´¥çš„æ–‡ä»¶...")
        
        try:
            self.failed_dir.mkdir(parents=True, exist_ok=True)
            
            # å†™å…¥é”™è¯¯æ—¥å¿—
            log_file = self.failed_dir / "error_log.txt"
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write(f"æ–‡ä»¶æ•´ç†é”™è¯¯æ—¥å¿— - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("="*80 + "\n\n")
                for file_path, error in self.failed_files:
                    f.write(f"æ–‡ä»¶: {file_path}\n")
                    f.write(f"é”™è¯¯: {error}\n")
                    f.write("-"*80 + "\n")
            
            print(f"é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
            print(f"å¤±è´¥æ–‡ä»¶ç›®å½•: {self.failed_dir}")
            
            # å°è¯•ç§»åŠ¨å¤±è´¥çš„æ–‡ä»¶
            moved = 0
            for file_path, error in self.failed_files:
                try:
                    src = Path(file_path)
                    if src.exists():
                        dst = self.failed_dir / src.name
                        # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
                        if dst.exists():
                            timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
                            dst = self.failed_dir / f"{src.stem}_{timestamp}{src.suffix}"
                        shutil.move(str(src), str(dst))
                        moved += 1
                except Exception as e:
                    pass  # å¦‚æœç§»åŠ¨å¤±è´¥ï¼Œå¿½ç•¥
            
            if moved > 0:
                print(f"å·²å°† {moved} ä¸ªå¤±è´¥æ–‡ä»¶ç§»åŠ¨åˆ°: {self.failed_dir}")
        
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºå¤±è´¥æ–‡ä»¶ç›®å½• - {e}")
    
    def remove_duplicates_from_organized(self):
        """ä»å·²æ•´ç†çš„æ–‡ä»¶ä¸­åˆ é™¤é‡å¤æ–‡ä»¶ï¼ˆä»…åœ¨åŒä¸€æ–‡ä»¶å¤¹å†…å¯¹æ¯”ï¼‰"""
        print("æ­£åœ¨æ‰«æå·²æ•´ç†çš„æ–‡ä»¶å¹¶è®¡ç®—å“ˆå¸Œå€¼...")
        
        # æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„æ‰«æ
        folders = []
        for year_dir in self.target_dir.iterdir():
            if year_dir.is_dir():
                for month_dir in year_dir.iterdir():
                    if month_dir.is_dir():
                        folders.append(month_dir)
        
        if not folders:
            print("\næœªæ‰¾åˆ°å·²æ•´ç†çš„æ–‡ä»¶å¤¹")
            return
        
        print(f"æ‰¾åˆ° {len(folders)} ä¸ªæ–‡ä»¶å¤¹ï¼Œå¼€å§‹é€ä¸ªæ–‡ä»¶å¤¹æ£€æµ‹é‡å¤...")
        
        total_deleted = 0
        total_size_saved = 0
        total_duplicates_found = 0
        
        for folder in folders:
            # è·å–å½“å‰æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
            folder_files = [f for f in folder.iterdir() if f.is_file()]
            
            if len(folder_files) < 2:
                continue  # å°‘äº2ä¸ªæ–‡ä»¶ï¼Œä¸å¯èƒ½æœ‰é‡å¤
            
            # è®¡ç®—å½“å‰æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰æ–‡ä»¶çš„å“ˆå¸Œå€¼
            hash_map = defaultdict(list)
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {executor.submit(self.calculate_hash_worker, f): f for f in folder_files}
                
                for future in as_completed(futures):
                    file_path, file_hash = future.result()
                    if file_hash:
                        hash_map[file_hash].append(file_path)
            
            # æŸ¥æ‰¾å½“å‰æ–‡ä»¶å¤¹ä¸­çš„é‡å¤æ–‡ä»¶
            folder_duplicates = {k: v for k, v in hash_map.items() if len(v) > 1}
            
            if not folder_duplicates:
                continue
            
            # æ˜¾ç¤ºå½“å‰æ–‡ä»¶å¤¹ä¿¡æ¯
            folder_dup_count = sum(len(files) - 1 for files in folder_duplicates.values())
            total_duplicates_found += folder_dup_count
            print(f"\n{folder.parent.name}/{folder.name} - å‘ç° {len(folder_duplicates)} ç»„é‡å¤æ–‡ä»¶ï¼Œå…± {folder_dup_count} ä¸ªé‡å¤é¡¹")
            
            # åˆ é™¤é‡å¤æ–‡ä»¶
            for file_hash, file_list in folder_duplicates.items():
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ—©çš„
                sorted_files = sorted(file_list, key=lambda x: x.stat().st_mtime)
                keep_file = sorted_files[0]
                delete_files = sorted_files[1:]
                
                keep_time = datetime.fromtimestamp(keep_file.stat().st_mtime)
                print(f"  ä¿ç•™: {keep_file.name} ({keep_time.strftime('%Y-%m-%d %H:%M:%S')})")
                
                for dup_file in delete_files:
                    try:
                        file_size = dup_file.stat().st_size
                        dup_time = datetime.fromtimestamp(dup_file.stat().st_mtime)
                        dup_file.unlink()
                        total_deleted += 1
                        total_size_saved += file_size
                        print(f"    åˆ é™¤: {dup_file.name} ({dup_time.strftime('%Y-%m-%d %H:%M:%S')}) - {self.format_size(file_size)}")
                    except Exception as e:
                        print(f"    é”™è¯¯: æ— æ³•åˆ é™¤ {dup_file.name} - {e}")
        
        if total_duplicates_found == 0:
            print("\næœªå‘ç°é‡å¤æ–‡ä»¶")
        else:
            print(f"\nå®Œæˆ! åˆ é™¤äº† {total_deleted} ä¸ªé‡å¤æ–‡ä»¶ï¼ŒèŠ‚çœç©ºé—´ {self.format_size(total_size_saved)}")


def main():
    parser = argparse.ArgumentParser(
        description='æ ¹æ®æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ•´ç†æ–‡ä»¶åˆ°å¹´/æœˆæ–‡ä»¶å¤¹ï¼Œå¹¶æŒ‰æ—¥æœŸåºå·é‡å‘½å',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python file_organizer.py D:\\RecoveredFiles
  python file_organizer.py D:\\RecoveredFiles -o D:\\Organized
  python file_organizer.py D:\\RecoveredFiles --dry-run
  python file_organizer.py D:\\RecoveredFiles --remove-duplicates
  python file_organizer.py D:\\RecoveredFiles --remove-duplicates --dry-run
        """
    )
    
    parser.add_argument('source', help='æºæ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆé»˜è®¤ä¸ºæºæ–‡ä»¶å¤¹ä¸‹çš„organizedç›®å½•ï¼‰')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…ç§»åŠ¨æ–‡ä»¶')
    parser.add_argument('--remove-duplicates', action='store_true', help='åˆ é™¤é‡å¤æ–‡ä»¶ï¼ˆåŸºäºæ–‡ä»¶å†…å®¹å“ˆå¸Œæ¯”å¯¹ï¼‰')
    parser.add_argument('--include-brand', action='store_true', help='åœ¨æ–‡ä»¶åä¸­åŒ…å«ç›¸æœºå“ç‰Œï¼ˆéœ€è¦Pillowåº“ï¼‰')
    parser.add_argument('--threads', type=int, default=4, help='çº¿ç¨‹æ•°ï¼ˆé»˜è®¤4ï¼‰')
    
    args = parser.parse_args()
    
    try:
        organizer = FileOrganizer(args.source, args.output, args.dry_run, args.remove_duplicates, args.include_brand, args.threads)
        organizer.organize_files()
        print("\nç¨‹åºå·²æ­£å¸¸é€€å‡º")
        return 0
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 1
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
