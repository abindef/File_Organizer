# 文件整理工具

根据文件的最后修改时间，自动将文件整理到 `年/月` 文件夹结构中，并按照 `yyyymmdd00x` 格式重命名。

## 功能特点

- 📁 自动按年/月创建文件夹结构
- 📅 根据文件最后修改时间进行分类
- 🔄 按日期和序号重命名文件（格式：yyyymmdd001, yyyymmdd002...）
- 🔍 **智能去重**：基于文件内容哈希值识别并删除重复文件
- 👀 支持预览模式，先查看效果再执行
- 🔒 保留原文件扩展名
- 🔢 **智能序号调整**：当目标文件已存在时，自动递增序号，确保所有文件都能被整理
- 📷 **品牌识别**：可选从图片EXIF数据中提取相机品牌并添加到文件名中
- 🗂️ **递归处理**：自动遍历所有子文件夹，处理所有层级的文件
- ⚡ **多线程处理**：支持多线程并发处理，大幅提升处理速度
- 📊 **批量日志**：每1000个文件打印一次进度，避免日志过多
- 🛡️ **错误容错**：自动跳过无法访问的文件或文件夹，继续处理其他文件
- 📝 **失败文件处理**：自动记录错误日志并将失败文件移动到专门目录
- 🔄 **优化工作流**：先整理文件，最后再删除重复文件，避免影响整理性能

## 使用方法

### 基本用法

```bash
python file_organizer.py <源文件夹路径>
```

这将在源文件夹下创建 `organized` 目录，并将文件整理到其中。

### 指定输出目录

```bash
python file_organizer.py <源文件夹路径> -o <输出文件夹路径>
python file_organizer.py H:\picture\failed_files -o H:\picture\organized --remove-duplicates --include-brand --threads 8
```

### 预览模式（推荐首次使用）

```bash
python file_organizer.py <源文件夹路径> --dry-run
```

预览模式会显示将要执行的操作，但不会实际移动文件。

### 删除重复文件

```bash
python file_organizer.py <源文件夹路径> --remove-duplicates
```

程序会通过计算文件的MD5哈希值来识别内容完全相同的文件，即使文件名不同也能准确识别。保留修改时间最早的文件，删除其他重复项。

### 包含相机品牌信息

```bash
python file_organizer.py <源文件夹路径> --include-brand
```

程序会从图片的EXIF数据中提取相机品牌（如Canon、Nikon、Sony等），并将品牌名称添加到文件名中。

**注意**：此功能需要安装Pillow库：
```bash
pip install Pillow
```

### 多线程处理

```bash
# 使用默认4个线程
python file_organizer.py <源文件夹路径>

# 指定线程数（建议根据CPU核心数调整）
python file_organizer.py <源文件夹路径> --threads 8
```

多线程处理可以显著提升大量文件的处理速度，特别是在：
- 计算文件哈希值（去重功能）
- 提取EXIF数据（品牌识别功能）
- 分析文件信息

**建议线程数**：
- 小于1000个文件：使用默认4线程
- 1000-10000个文件：使用8线程
- 超过10000个文件：使用8-16线程

### 组合使用

```bash
# 预览整理效果
python file_organizer.py <源文件夹路径> --dry-run

# 整理文件
python file_organizer.py <源文件夹路径>

# 整理+去重（先整理，完成后再删除重复文件）
python file_organizer.py <源文件夹路径> --remove-duplicates

# 整理+去重+品牌识别
python file_organizer.py <源文件夹路径> --remove-duplicates --include-brand

# 使用8个线程加速处理
python file_organizer.py <源文件夹路径> --threads 8 --remove-duplicates
```

## 使用示例

### 示例 1：整理恢复的文件

```bash
python file_organizer.py D:\RecoveredFiles
```

结果：
```
D:\RecoveredFiles\organized\
├── 2023\
│   ├── 01\
│   │   ├── 20230115001.jpg
│   │   ├── 20230115002.jpg
│   │   └── 20230128001.pdf
│   └── 02\
│       ├── 20230201001.docx
│       └── 20230215001.jpg
└── 2024\
    └── 12\
        ├── 20241201001.png
        └── 20241201002.png
```

### 示例 2：指定输出目录

```bash
python file_organizer.py D:\RecoveredFiles -o D:\OrganizedFiles
```

### 示例 3：先预览再执行

```bash
# 第一步：预览
python file_organizer.py D:\RecoveredFiles --dry-run

# 第二步：确认无误后执行
python file_organizer.py D:\RecoveredFiles
```

### 示例 4：整理并删除重复文件

```bash
# 整理文件并删除重复文件（先整理，完成后再去重）
python file_organizer.py D:\RecoveredFiles --remove-duplicates
```

输出示例：
```
正在递归扫描文件夹...
找到 150 个文件，开始分析...
分析完成，共 150 个文件

处理 2023年1月 的 50 个文件...
  已处理 1000/1500 个文件...

完成! 成功整理 150 个文件

==================================================
整理完成，开始检测并删除重复文件...
==================================================

正在扫描已整理的文件并计算哈希值...
找到 150 个文件，开始计算哈希值...
已扫描 150 个文件

发现 5 组重复文件，共 8 个重复项

保留: 20230115001.jpg (2023-01-15 10:30:25)
  删除: 20230115002.jpg (2023-01-15 10:35:10) - 2.45 MB
  删除: 20230115003.jpg (2023-01-15 11:20:00) - 2.45 MB

完成! 删除了 8 个重复文件，节省空间 18.50 MB
```

### 示例 5：完整工作流

```bash
# 一次性完成整理和去重（带品牌识别和多线程）
python file_organizer.py H:\picture\JPEG --remove-duplicates --include-brand --threads 8 -o D:\OrganizedPhotos
```

### 示例 6：序号冲突自动调整

当目标位置已存在文件时，程序会自动调整序号：

```
处理 2023年1月 的 5 个文件...
  IMG_001.jpg -> 2023/01/20230115001.jpg
  IMG_002.jpg -> 2023/01/20230115002.jpg
  序号冲突，自动调整: IMG_003.jpg -> 2023/01/20230115004.jpg (序号 3 -> 4)
  IMG_004.jpg -> 2023/01/20230115005.jpg

完成! 成功整理 4 个文件
其中 1 个文件因序号冲突自动调整了序号
```

这个功能确保了：
- 所有文件都能被成功整理，不会被跳过
- 多次运行程序时，新文件会自动接着已有文件的序号继续排列
- 不会覆盖任何已存在的文件

### 示例 7：包含相机品牌信息

```bash
python file_organizer.py H:\picture\JPEG --include-brand
```

输出示例：
```
处理 2023年5月 的 10 个文件...
  DSC_0001.jpg -> 2023/05/Nikon_20230515001.jpg
    [品牌: Nikon]
  IMG_2345.jpg -> 2023/05/Canon_20230515002.jpg
    [品牌: Canon]
  PHOTO_001.jpg -> 2023/05/Sony_20230515003.jpg
    [品牌: Sony]
  random_pic.jpg -> 2023/05/20230515004.jpg

完成! 成功整理 10 个文件
```

### 示例 8：大量文件处理（批量日志）

处理大量文件时，程序会每1000个文件打印一次进度：

```bash
python file_organizer.py H:\picture\嘟嘟 -o H:\picture\organized --threads 8
```

输出示例：
```
正在递归扫描文件夹...
找到 5000 个文件，开始分析...
  已分析 1000/5000 个文件...
  已分析 2000/5000 个文件...
  已分析 3000/5000 个文件...
  已分析 4000/5000 个文件...
分析完成，共 5000 个文件

处理 2024年11月 的 2315 个文件...
  已处理 1000/2315 个文件...
  已处理 2000/2315 个文件...
  序号冲突，自动调整: xxx.jpg -> 2024/11/20241124185.jpg (序号 184 -> 185)

完成! 成功整理 5000 个文件

处理 3 个失败的文件...
错误日志已保存到: H:\picture\failed_files\error_log.txt
失败文件目录: H:\picture\failed_files
已将 3 个失败文件移动到: H:\picture\failed_files
```

文件命名格式（包含品牌）：`Brand_yyyymmdd00x.ext`
- 例如：`Canon_20230515001.jpg`、`Nikon_20231225003.jpg`
- 如果无法提取品牌信息，则使用标准格式：`20230515004.jpg`

## 错误处理

程序会自动处理各种错误情况：

- **路径无法访问**：自动跳过并统计数量
- **文件移动失败**：记录到错误日志
- **哈希计算失败**：跳过该文件的去重检测
- **I/O设备错误**：跳过问题文件，继续处理其他文件

所有失败的文件都会被记录到 `failed_files/error_log.txt`，方便后续排查。

## 命名规则说明

**标准格式**：`yyyymmdd00x.ext`

- `yyyy`：4位年份
- `mm`：2位月份
- `dd`：2位日期
- `00x`：3位序号（001, 002, 003...）
- `.ext`：原文件扩展名

例如：
- `20231225001.jpg` - 2023年12月25日的第1个文件
- `20231225002.jpg` - 2023年12月25日的第2个文件
- `20240101001.pdf` - 2024年1月1日的第1个文件

**包含品牌格式**（使用 `--include-brand` 参数）：`Brand_yyyymmdd00x.ext`

- 品牌名称放在文件名最前面
- 品牌名称从图片EXIF数据中提取（相机制造商）

例如：
- `Canon_20231225001.jpg` - Canon相机拍摄的照片
- `Nikon_20231225002.jpg` - Nikon相机拍摄的照片
- `Sony_20231225003.jpg` - Sony相机拍摄的照片
- `20231225004.jpg` - 无法识别品牌的文件（使用标准格式）

## 工作流程说明

### 处理顺序

程序采用**先整理后去重**的策略，避免去重影响整理性能：

1. **扫描文件**：递归扫描所有子文件夹，跳过无法访问的路径
2. **分析文件**：多线程提取文件信息（日期、品牌等）
3. **整理文件**：按年/月组织文件并重命名
4. **处理失败**：记录错误日志，移动失败文件到专门目录
5. **删除重复**：如果启用了 `--remove-duplicates`，在整理完成后扫描并删除重复文件

### 重复文件检测

程序使用 **MD5 哈希算法**比对文件内容：
- 即使文件名完全不同，只要内容相同就会被识别为重复
- 不受文件名、创建时间影响，只比对实际内容
- 对于重复文件组，保留**修改时间最早**的文件
- **在已整理的文件中**进行去重，不影响整理过程的性能
- **仅在同一文件夹内对比**：只比较同一年/月文件夹中的文件，不跨文件夹比较

### 失败文件处理

遇到无法处理的文件时：
1. 自动跳过该文件，继续处理其他文件
2. 记录错误信息到 `failed_files/error_log.txt`
3. 尝试将失败文件移动到 `failed_files` 目录
4. 在处理完成后显示失败文件统计

## 注意事项

1. ⚠️ **建议先使用 `--dry-run` 预览模式**查看效果
2. 🗑️ **删除重复文件是不可逆操作**，建议先预览确认
3. 📦 程序会移动文件，不是复制，请确保有备份
4. 🗂️ **自动递归处理所有子文件夹**，包括多层嵌套的文件夹
5. 🔢 如果目标位置已存在同名文件，程序会自动递增序号，确保所有文件都能被处理
6. 💾 确保有足够的磁盘空间
7. 🔐 重复检测基于文件内容，大文件可能需要较长时间
8. ⚡ 使用多线程可以加速处理，但会增加CPU和内存使用
9. 🛡️ 遇到无法访问的文件或文件夹会自动跳过并继续处理
10. 📝 失败的文件会被记录到错误日志并移动到 `failed_files` 目录
11. 🔄 去重操作在文件整理完成后进行，不影响整理性能

## 系统要求

- Python 3.6 或更高版本
- 基础功能无需额外依赖包（仅使用Python标准库）
- **可选依赖**：Pillow库（用于提取图片EXIF数据中的品牌信息）
  ```bash
  pip install Pillow
  ```

## 性能优化建议

**处理大量文件时的建议：**

1. **使用多线程**：根据CPU核心数调整线程数（建议8-16线程）
   ```bash
   python file_organizer.py <源文件夹> --threads 8
   ```

2. **分批处理**：如果文件数量超过10万，建议分批处理

3. **先整理后去重**：程序已自动优化为先整理再去重，无需手动操作

4. **检查失败文件**：处理完成后查看 `failed_files/error_log.txt` 了解失败原因

5. **磁盘性能**：使用SSD可以显著提升处理速度

## 常见问题

**Q: 文件会被复制还是移动？**  
A: 文件会被移动（剪切），不是复制。

**Q: 如果目标文件夹已存在同名文件会怎样？**  
A: 程序会自动递增序号，例如如果 20231225001.jpg 已存在，则会使用 20231225002.jpg，以此类推，确保所有文件都能被整理。

**Q: 可以处理子文件夹中的文件吗？**  
A: 可以。程序会自动递归扫描所有子文件夹（包括多层嵌套），处理所有找到的文件。

**Q: 如何撤销操作？**  
A: 程序不提供撤销功能，建议先使用 `--dry-run` 预览，或在操作前备份重要文件。

**Q: 如何判断文件是否重复？**  
A: 程序通过计算文件的MD5哈希值来判断，只要文件内容完全相同就会被识别为重复，与文件名无关。

**Q: 删除重复文件时会保留哪一个？**  
A: 保留修改时间最早的文件，删除其他较新的副本。这样可以保留原始文件。

**Q: 去重和整理的顺序是什么？**  
A: 程序会**先整理文件**，整理完成后再**删除重复文件**。这样可以避免去重过程影响整理性能，特别是处理大量文件时效果明显。

**Q: 为什么要先整理再去重？**  
A: 因为计算文件哈希值（去重）比较耗时。先整理文件可以让大部分文件快速完成组织，然后在已整理的文件中进行去重，提高整体效率。

**Q: 去重时会跨文件夹比较吗？**  
A: 不会。去重只在**同一个年/月文件夹内**进行比较，不会跨文件夹比较。例如，2021年3月的文件只会和同月的其他文件比较，不会和2021年4月或其他月份的文件比较。这样更合理且效率更高。

**Q: 如果多次运行程序会怎样？**  
A: 程序会智能识别已存在的文件并自动调整序号，不会覆盖或丢失任何文件。

**Q: 品牌识别支持哪些图片格式？**  
A: 支持常见的图片格式：JPG、JPEG、PNG、TIFF、HEIC、HEIF等。品牌信息从图片的EXIF数据中提取。

**Q: 如果图片没有EXIF数据或无法识别品牌会怎样？**  
A: 程序会使用标准命名格式（不包含品牌名称），不会影响文件整理过程。

**Q: 可以自定义品牌名称吗？**  
A: 当前版本自动从EXIF数据提取，通常为相机制造商名称（如Canon、Nikon、Sony等）。

**Q: 子文件夹的结构会被保留吗？**  
A: 不会。程序会将所有子文件夹中的文件提取出来，按照日期重新组织到年/月的文件夹结构中，原有的文件夹结构不会被保留。

**Q: 如果子文件夹很多，处理会很慢吗？**  
A: 程序支持多线程处理，可以显著提升速度。使用 `--threads 8` 参数可以加速处理。程序会每1000个文件显示一次进度。

**Q: 遇到无法访问的文件夹怎么办？**  
A: 程序会自动跳过无法访问的文件或文件夹（如I/O错误、权限问题），并在开始时显示跳过的路径列表，然后继续处理其他文件。

**Q: 如何提高处理速度？**  
A: 使用 `--threads` 参数增加线程数。建议根据CPU核心数设置，通常8-16个线程即可。注意线程数过多可能导致性能下降。

**Q: 为什么日志输出变少了？**  
A: 为了避免日志过多，程序每1000个文件才打印一次进度。只有序号冲突等特殊情况会单独打印。

**Q: 失败的文件会怎么处理？**  
A: 失败的文件会被记录到 `failed_files/error_log.txt` 日志文件中，并尝试移动到 `failed_files` 目录。这样可以单独查看和处理这些文件。

**Q: 如果遇到大量无法访问的文件怎么办？**  
A: 程序会自动跳过这些文件并显示跳过的数量。这些文件通常是由于路径过长、特殊字符或I/O设备错误导致的。程序会继续处理其他可访问的文件。

**Q: 为什么去重只在同一文件夹内进行？**  
A: 因为不同时间段（年/月）的文件通常不应该互相比较。同一张照片出现在不同月份是合理的（例如备份或整理），只有同一个月内的重复文件才需要删除。这样可以避免误删，同时大幅提升去重效率。
